
@article{li_digital_2024,
	title = {Digital {Twin}‐{Enabled} {Deep} {Reinforcement} {Learning} for {Safety}‐{Guaranteed} {Flocking} {Motion} of {\textless}span style="font-variant:small-caps;"{\textgreater}{UAV}{\textless}/span{\textgreater} {Swarm}},
	volume = {35},
	issn = {2161-3915, 2161-3915},
	shorttitle = {Digital {Twin}‐{Enabled} {Deep} {Reinforcement} {Learning} for {Safety}‐{Guaranteed} {Flocking} {Motion} of {\textless}span style="font-variant},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/ett.70011},
	doi = {10.1002/ett.70011},
	abstract = {Multi-agent deep reinforcement learning (MADRL) has become a typical paradigm for the flocking motion of UAV swarm in dynamic, stochastic environments. However, sim-to-real problems, such as reality gap, training efficiency, and safety issues, restrict the application of MADRL in flocking motion scenarios. To address these problems, we first propose a digital twin (DT)-enabled training framework. With the assistance of high-fidelity digital twin simulation, effective policies can be efficiently trained. Based on the multi-agent proximal policy optimization (MAPPO) algorithm, we then design the learning approach for flocking motion with matching observation space, action space, and reward function. Afterward, we employ a distributed flocking center estimation algorithm based on position consensus. The estimated center is used as a policy input to improve the aggregation behavior. Moreover, we introduce a repulsion scheme, which applies an additional repulsion force to the action to prevent UAVs from colliding with neighbors and obstacles. Simulation results show that our method performs well in maintaining flocking formation and avoiding collisions, and has better decision-making ability in near-realistic environments.},
	language = {en},
	number = {11},
	urldate = {2025-05-06},
	journal = {Transactions on Emerging Telecommunications Technologies},
	author = {Li, Zhilin and Lei, Lei and Shen, Gaoqing and Liu, Xiaochang and Liu, Xiaojiao},
	month = nov,
	year = {2024},
	note = {TLDR: This work proposes a digital twin (DT)‐enabled training framework, and introduces a repulsion scheme, which applies an additional repulsion force to the action to prevent UAVs from colliding with neighbors and obstacles.},
	pages = {e70011},
	file = {PDF:files/18/Li 等 - 2024 - Digital Twin‐Enabled Deep Reinforcement Learning for Safety‐Guaranteed Flocking Motion of span styl.pdf:application/pdf},
}

@article{hu_multitask_2020,
	title = {Multi‐task deep learning with optical flow features for self‐driving cars},
	volume = {14},
	issn = {1751-956X, 1751-9578},
	url = {https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-its.2020.0439},
	doi = {10.1049/iet-its.2020.0439},
	abstract = {The control of self-driving cars has received growing attention recently. Although existing research shows promising results in the vehicle control using video from a monocular dash camera, there has been very limited work on directly learning vehicle control from motion-based cues. Such cues are powerful features for visual representations, as they encode the perpixel movement between two consecutive images, allowing a system to effectively map the features into the control signal. The authors propose a new framework that exploits the use of a motion-based feature known as optical flow extracted from the dash camera and demonstrates that such a feature is effective in significantly improving the accuracy of the control signals. The proposed framework involves two main components. The flow predictor, as a self-supervised deep network, models the underlying scene structure from consecutive frames and generates the optical flow. The controller, as a supervised multi-task deep network, predicts both steer angle and speed. The authors demonstrate that the proposed framework using the optical flow features can effectively predict control signals from a dash camera video. Using the Cityscapes data set, the authors validate that the system prediction has errors as low as 0.0130 rad/s on steer angle and 0.0615 m/s on speed, outperforming existing research.},
	language = {en},
	number = {13},
	urldate = {2025-05-07},
	journal = {IET Intelligent Transport Systems},
	author = {Hu, Yuan and Shum, Hubert P. H. and Ho, Edmond S. L.},
	month = dec,
	year = {2020},
	note = {TLDR: A new framework that exploits the use of a motion-based feature known as optical feature extracted from the dash camera is proposed, and it is demonstrated that such a feature is effective in signiﬁcantly improving the accuracy of the control signals.},
	pages = {1845--1854},
	file = {PDF:files/21/Hu 等 - 2020 - Multi‐task deep learning with optical flow features for self‐driving cars.pdf:application/pdf},
}

@article{fu_reward_2014,
	title = {A {Reward} {Optimization} {Method} {Based} on {Action} {Subrewards} in {Hierarchical} {Reinforcement} {Learning}},
	volume = {2014},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2356-6140, 1537-744X},
	url = {http://www.hindawi.com/journals/tswj/2014/120760/},
	doi = {10.1155/2014/120760},
	abstract = {Reinforcement learning (RL) is one kind of interactive learning methods. Its main characteristics are “trial and error” and “related reward.” A hierarchical reinforcement learning method based on action subrewards is proposed to solve the problem of “curse of dimensionality,” which means that the states space will grow exponentially in the number of features and low convergence speed. The method can reduce state spaces greatly and choose actions with favorable purpose and efficiency so as to optimize reward function and enhance convergence speed. Apply it to the online learning in Tetris game, and the experiment result shows that the convergence speed of this algorithm can be enhanced evidently based on the new method which combines hierarchical reinforcement learning algorithm and action subrewards. The “curse of dimensionality” problem is also solved to a certain extent with hierarchical method. All the performance with different parameters is compared and analyzed as well.},
	language = {en},
	urldate = {2025-05-07},
	journal = {The Scientific World Journal},
	author = {Fu, Yuchen and Liu, Quan and Ling, Xionghong and Cui, Zhiming},
	year = {2014},
	pages = {1--6},
	file = {PDF:files/23/Fu 等 - 2014 - A Reward Optimization Method Based on Action Subrewards in Hierarchical Reinforcement Learning.pdf:application/pdf},
}

@article{kumar_enhancing_2025,
	title = {Enhancing {Autonomous} {Vehicle} {Navigation} in {Complex} {Environment} {With} {Semantic} {Proto}‐{Reinforcement} {Learning}},
	issn = {1556-4959, 1556-4967},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/rob.22506},
	doi = {10.1002/rob.22506},
	abstract = {Despite great progress in autonomous vehicle (AV) navigation, the technical challenges within this space are still considerable when it comes to successful integration of AVs into complex real‐world environments. To tackle these challenges, this paper presents a new semantic proto‐reinforcement learning (SP‐RL) method for dynamic path planning and real‐time obstacle avoidance of an autonomous car that can be adapted to various weather conditions in localization‐deficient environments, while predicting the human intentions of different shapes on road. This approach seeks to improve navigation capability of AV in dynamic and unstructured environments, as well as to address real‐time detection and avoidance response for obstacles more promptly while being able to adapt its decision‐making system based on weather condition by using the semantic graph network (SGN) within segmentation process therefore enhanced version configured with prototype‐based reinforcement learning (PRL). This innovation is new competitive edge compared previous existing approaches. Dynamic SGN is used to segment challenging 3D and free space environments, so that the AV can comprehend highly unintuitive areas like parking lots, construction site conditionals, or off‐road scenarios. At the same time, PRL is used to help real‐time decision‐making so the AV can quickly and precisely respond unexpected obstacles or changing environments. This approach is confirmed to be effective by extensive testing in the CARLA simulation environment, showing substantial improvement of AV navigation capability. This work demonstrates an exciting step towards solving the most fundamental problems faced by autonomous vehicles and could help to ensure that future AV systems are safer, more robust, and adaptable than current ones. It is appliable for urban areas which represent high volumes of pedestrians and vehicles, industrial sites with changing conditions that may be unpredictable at times or the challenging off‐road areas in rural geographies where typical terrains are rough, uneven, rugged yet not wellstructured. The model is robust to diverse weather conditions that make AV operations more reliable and safer. The model was tested on root mean square error (RMSE), computational time, no crash, obstacles avoidance, and success rate obtaining overall 98\%.},
	language = {en},
	urldate = {2025-05-07},
	journal = {Journal of Field Robotics},
	author = {Kumar, G. Anand and Mohiddin, Md. Khaja and Mishra, Shashi Kant and Verma, Abhishek and Sharma, Mousam and Naresh, A.},
	month = jan,
	year = {2025},
	note = {TLDR: A new semantic proto‐reinforcement learning (SP‐RL) method for dynamic path planning and real‐time obstacle avoidance of an autonomous car that can be adapted to various weather conditions in localization‐deficient environments, while predicting the human intentions of different shapes on road.},
	pages = {rob.22506},
	file = {PDF:files/27/Kumar 等 - 2025 - Enhancing Autonomous Vehicle Navigation in Complex Environment With Semantic Proto‐Reinforcement Lea.pdf:application/pdf},
}

@article{lu_multifeature_2021,
	title = {Multifeature {Fusion} {Human} {Motion} {Behavior} {Recognition} {Algorithm} {Using} {Deep} {Reinforcement} {Learning}},
	volume = {2021},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {1875-905X, 1574-017X},
	url = {https://www.hindawi.com/journals/misy/2021/2199930/},
	doi = {10.1155/2021/2199930},
	abstract = {Through the recognition and analysis of human motion information, the actual motion state of human body can be obtained. However, the multifeature fusion of human behavior has limitations in recognition accuracy and robustness. Combined with deep reinforcement learning, multifeature fusion human behavior recognition is studied and we proposed a multifeature fusion human behavior recognition algorithm using deep reinforcement learning. Firstly, several typical human behavior data sets are selected as the research data in the benchmark data set. In the selected data sets, the behavior category contained in each video is the same behavior, and there are category tags. Secondly, the attention model is constructed. In the deep reinforcement learning network, the small sampling area is used as the model input. Finally, the corresponding position of the next visual area is estimated according to the time series information obtained after the input. The human behavior recognition algorithm based on deep reinforcement learning multifeature fusion is completed. The results show that the average accuracy of multifeature fusion of the algorithm is about 95\%, the human behavior recognition effect is good, the identification accuracy rate is as high as about 98\% and passed the camera movement impact performance test and the algorithm robustness, and the average time consumption of the algorithm is only 12.7 s, which shows that the algorithm has very broad application prospects.},
	language = {en},
	urldate = {2025-05-07},
	journal = {Mobile Information Systems},
	author = {Lu, Chengkun},
	editor = {Fiaidhi, Jinan},
	month = nov,
	year = {2021},
	note = {TLDR: The results show that the average accuracy of multifeature fusion of the algorithm is about 95\%, the human behavior recognition effect is good, the identification accuracy rate is as high as about 98\% and passed the camera movement impact performance test and the algorithm robustness, and the average time consumption of the algorithms is only 12.7 s, which shows that the algorithm has very broad application prospects.},
	pages = {1--11},
	file = {PDF:files/29/Lu - 2021 - Multifeature Fusion Human Motion Behavior Recognition Algorithm Using Deep Reinforcement Learning.pdf:application/pdf},
}

@article{zhou_safe_2023,
	title = {A safe reinforcement learning approach for autonomous navigation of mobile robots in dynamic environments},
	issn = {2468-2322, 2468-2322},
	url = {https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/cit2.12269},
	doi = {10.1049/cit2.12269},
	abstract = {When deploying mobile robots in real‐world scenarios, such as airports, train stations, hospitals, and schools, collisions with pedestrians are intolerable and catastrophic. Motion safety becomes one of the most fundamental requirements for mobile robots. However, until now, efficient and safe robot navigation in such dynamic environments is still an open problem. The critical reason is that the inconsistency between navigation efficiency and motion safety is greatly intensified by the high dynamics and uncertainties of pedestrians. To face the challenge, this paper proposes a safe deep reinforcement learning algorithm named Conflict‐Averse Safe Reinforcement Learning (CASRL) for autonomous robot navigation in dynamic environments. Specifically, it first separates the collision avoidance sub‐task from the overall navigation task and maintains a safety critic to evaluate the safety/risk of actions. Later, it constructs two task‐specific but modelagnostic policy gradients for goal‐reaching and collision avoidance sub‐tasks to eliminate their mutual interference. Then, it further performs a conflict‐averse gradient manipulation to address the inconsistency between two sub‐tasks. Finally, extensive experiments are performed to evaluate the superiority of CASRL. Simulation results show an average 8.2\% performance improvement over the vanilla baseline in eight groups of dynamic environments, which is further extended to 13.4\% in the most challenging group. Besides, forty real‐world experiments fully illustrated that the CASRL could be successfully deployed on a real robot.},
	language = {en},
	urldate = {2025-05-07},
	journal = {CAAI Transactions on Intelligence Technology},
	author = {Zhou, Zhiqian and Ren, Junkai and Zeng, Zhiwen and Xiao, Junhao and Zhang, Xinglong and Guo, Xian and Zhou, Zongtan and Lu, Huimin},
	month = oct,
	year = {2023},
	note = {TLDR: A safe deep reinforcement learning algorithm named Conflict‐Averse Safe Reinforcement Learning (CASRL) for autonomous robot navigation in dynamic environments and performs a conflict‐averse gradient manipulation to address the inconsistency between two sub‐tasks.},
	pages = {cit2.12269},
	file = {PDF:files/31/Zhou 等 - 2023 - A safe reinforcement learning approach for autonomous navigation of mobile robots in dynamic environ.pdf:application/pdf},
}

@article{zhang_consistent_nodate,
	title = {Consistent {Cooperative} {Control} of {Multi}‐{Support} {Systems} {Using} {Multi}‐{Agent} {Reinforcement} {Learning} {With} {Attention} {Mechanism}},
	abstract = {The research aims to improve the coordinated management of solid filling support systems in mining to enhance efficiency, structural stability, and overall safety. Given the challenges posed by complex subterranean environments and stringent filling targets, current control methods are insufficient. This study proposes a cooperative control framework for multi-filling support systems, modeling the consistency control problem as a multi-agent reinforcement learning paradigm. Each filling unit acts as an agent, communicating its action state with others. A novel multi-agent evaluation approach with an attention mechanism is introduced to facilitate path tracking and cooperative control. Considering constraints such as thrust, path deviation, and target position, simulation results show that the agents effectively collaborate to achieve the cooperative control of multi-filling support groups in complex conditions, highlighting the significance of this approach.},
	language = {en},
	author = {Zhang, Zihang and Liu, Yang and Yang, Shangqing},
	file = {PDF:files/33/Zhang 等 - Consistent Cooperative Control of Multi‐Support Systems Using Multi‐Agent Reinforcement Learning Wit.pdf:application/pdf},
}

@article{shang_constraintbased_2023,
	title = {Constraint‐based multi‐agent reinforcement learning for collaborative tasks},
	volume = {34},
	issn = {1546-4261, 1546-427X},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/cav.2182},
	doi = {10.1002/cav.2182},
	abstract = {In order to be successfully executed, collaborative tasks performed by two agents often require a cooperative strategy to be learned. In this work, we propose a constraint-based multi-agent reinforcement learning approach called constrained multi-agent soft actor critic (C-MSAC) to train control policies for simulated agents performing collaborative multi-phase tasks. Given a task with n phases, the first n − 1 phases are treated as constraints for the final task phase objective, which is addressed with a centralized training and decentralized execution approach. We highlight our framework on a tray balancing task including two phases: tray lifting and cooperative tray control for target following. We evaluate our proposed approach and compare it against its unconstrained variant (MSAC). The performed comparisons show that C-MSAC leads to higher success rates, more robust control policies, and better generalization performance.},
	language = {en},
	number = {3-4},
	urldate = {2025-05-07},
	journal = {Computer Animation and Virtual Worlds},
	author = {Shang, Xiumin and Xu, Tengyu and Karamouzas, Ioannis and Kallmann, Marcelo},
	month = may,
	year = {2023},
	note = {TLDR: This work proposes a constraint‐based multi-agent reinforcement learning approach called constrained multi‐agent soft actor critic (C‐MSAC) to train control policies for simulated agents performing collaborative multi‐phase tasks.},
	pages = {e2182},
	file = {PDF:files/37/Shang 等 - 2023 - Constraint‐based multi‐agent reinforcement learning for collaborative tasks.pdf:application/pdf},
}

@article{kim_avoiding_2021,
	title = {Avoiding collaborative paradox in multi‐agent reinforcement learning},
	volume = {43},
	copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
	issn = {1225-6463, 2233-7326},
	url = {https://onlinelibrary.wiley.com/doi/10.4218/etrij.2021-0010},
	doi = {10.4218/etrij.2021-0010},
	abstract = {The collaboration productively interacting between multi-agents has become an emerging issue in real-world applications. In reinforcement learning, multi-agent environments present challenges beyond tractable issues in single-agent settings. This collaborative environment has the following highly complex attributes: sparse rewards for task completion, limited communications between each other, and only partial observations. In particular, adjustments in an agent's action policy result in a nonstationary environment from the other agent's perspective, which causes high variance in the learned policies and prevents the direct use of reinforcement learning approaches. Unexpected social loafing caused by high dispersion makes it difficult for all agents to succeed in collaborative tasks. Therefore, we address a paradox caused by the social loafing to significantly reduce total returns after a certain timestep of multi-agent reinforcement learning. We further demonstrate that the collaborative paradox in multi-agent environments can be avoided by our proposed effective early stop method leveraging a metric for social loafing.},
	language = {en},
	number = {6},
	urldate = {2025-05-07},
	journal = {ETRI Journal},
	author = {Kim, Hyunseok and Kim, Seonghyun and Lee, Donghun and Jang, Ingook},
	month = dec,
	year = {2021},
	note = {TLDR: This work addresses a paradox caused by the social loafing to significantly reduce total returns after a certain timestep of multi‐agent reinforcement learning and demonstrates that the collaborative paradox in multi-agent environments can be avoided by the proposed effective early stop method leveraging a metric for social loafed.},
	pages = {1004--1012},
	file = {PDF:files/39/Kim 等 - 2021 - Avoiding collaborative paradox in multi‐agent reinforcement learning.pdf:application/pdf},
}
@article{demir2024learning,
  title={Learning Soft Millirobot Multimodal Locomotion with Sim-to-Real Transfer},
  author={Demir, Sinan Ozgun and Tiryaki, Mehmet Efe and Karacakol, Alp Can and Sitti, Metin},
  year={2024},
  journal={Advanced Science},
  doi={10.1002/advs.202308881}
}
